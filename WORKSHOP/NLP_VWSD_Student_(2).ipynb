{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WoFHp_fyFRH"
      },
      "source": [
        "# Prerequisite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VAI_Ql30H7O"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGkc8cIgx7fr"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate transformers datasets[vision] evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATbmbFrr0KHm"
      },
      "source": [
        "## Import requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ihv4A8V0MmR"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from PIL import Image as PIL_Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import evaluate\n",
        "\n",
        "\n",
        "from datasets import Dataset, Features, Value, Image, ClassLabel, concatenate_datasets\n",
        "from transformers import AltCLIPProcessor, AltCLIPModel\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.models.altclip.modeling_altclip import AltCLIPEncoderLayer\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.modeling_outputs import BaseModelOutput, ModelOutput\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set some parameters to have determinism"
      ],
      "metadata": {
        "id": "jcFjv_J9_08p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPXCuXoX0SYU",
        "outputId": "4b7443ec-190c-421f-e990-293eb0e16efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
          ]
        }
      ],
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "PIL_Image.MAX_IMAGE_PIXELS = 933120000\n",
        "\n",
        "GLOBAL_SEED = 10\n",
        "\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "torch.manual_seed(GLOBAL_SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbWwTpJTqxgb"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STOtyslJs7JS"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBBUDnAEofmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1588d4b1-1912-4ee4-9af8-b7d70c86a1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-27 21:13:35--  https://cf-my.sharepoint.com/:u:/g/personal/camachocolladosj_cardiff_ac_uk/ERFsG4by92ZPuW1dQQGuLfcBzHifN-NX1tCL6s6g-9-RMw?download=1\n",
            "Resolving cf-my.sharepoint.com (cf-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to cf-my.sharepoint.com (cf-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/camachocolladosj_cardiff_ac_uk/Documents/semeval-2023-task-1-V-WSD-train-v1.zip?ga=1 [following]\n",
            "--2023-05-27 21:13:36--  https://cf-my.sharepoint.com/personal/camachocolladosj_cardiff_ac_uk/Documents/semeval-2023-task-1-V-WSD-train-v1.zip?ga=1\n",
            "Reusing existing connection to cf-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18353852513 (17G) [application/x-zip-compressed]\n",
            "Saving to: ‘train_trial.zip’\n",
            "\n",
            "train_trial.zip     100%[===================>]  17.09G  64.2MB/s    in 5m 28s  \n",
            "\n",
            "2023-05-27 21:19:04 (53.4 MB/s) - ‘train_trial.zip’ saved [18353852513/18353852513]\n",
            "\n",
            "--2023-05-27 21:19:04--  https://cf-my.sharepoint.com/:u:/g/personal/camachocolladosj_cardiff_ac_uk/ETXzWJCEdKJDtFluRbYGUGYBzfdnaeOuSwL5hiqCW-k38Q?download=1\n",
            "Resolving cf-my.sharepoint.com (cf-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to cf-my.sharepoint.com (cf-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/camachocolladosj_cardiff_ac_uk/Documents/semeval2023-visualwsd-test_images_v1_notresized.zip?ga=1 [following]\n",
            "--2023-05-27 21:19:05--  https://cf-my.sharepoint.com/personal/camachocolladosj_cardiff_ac_uk/Documents/semeval2023-visualwsd-test_images_v1_notresized.zip?ga=1\n",
            "Reusing existing connection to cf-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10399295942 (9.7G) [application/x-zip-compressed]\n",
            "Saving to: ‘test_images.zip’\n",
            "\n",
            "test_images.zip      39%[======>             ]   3.85G  6.87MB/s    in 5m 24s  \n",
            "\n",
            "2023-05-27 21:24:30 (12.2 MB/s) - Read error at byte 4135675047/10399295942 (Connection reset by peer). Retrying.\n",
            "\n",
            "--2023-05-27 21:24:31--  (try: 2)  https://cf-my.sharepoint.com/personal/camachocolladosj_cardiff_ac_uk/Documents/semeval2023-visualwsd-test_images_v1_notresized.zip?ga=1\n",
            "Connecting to cf-my.sharepoint.com (cf-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 PARTIAL CONTENT\n",
            "Length: 10399295942 (9.7G), 6263620895 (5.8G) remaining [application/x-zip-compressed]\n",
            "Saving to: ‘test_images.zip’\n",
            "\n",
            "test_images.zip     100%[+++++++============>]   9.68G  65.0MB/s    in 1m 51s  \n",
            "\n",
            "2023-05-27 21:26:22 (54.0 MB/s) - ‘test_images.zip’ saved [10399295942/10399295942]\n",
            "\n",
            "--2023-05-27 21:26:22--  https://docs.google.com/uc?export=download&id=10vDZsY0EhzvFFR8IF-3P_2ApOF0GIMML&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.107.102, 142.250.107.113, 142.250.107.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.107.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0o-48-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n3092te6cp9r1d9uvu08771joq5dkimo/1685222775000/04589392675467887255/*/10vDZsY0EhzvFFR8IF-3P_2ApOF0GIMML?e=download&uuid=b3c17249-670e-4d17-8e63-bf9250f31e8d [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-27 21:26:23--  https://doc-0o-48-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n3092te6cp9r1d9uvu08771joq5dkimo/1685222775000/04589392675467887255/*/10vDZsY0EhzvFFR8IF-3P_2ApOF0GIMML?e=download&uuid=b3c17249-670e-4d17-8e63-bf9250f31e8d\n",
            "Resolving doc-0o-48-docs.googleusercontent.com (doc-0o-48-docs.googleusercontent.com)... 172.253.117.132, 2607:f8b0:400e:c0a::84\n",
            "Connecting to doc-0o-48-docs.googleusercontent.com (doc-0o-48-docs.googleusercontent.com)|172.253.117.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested range not satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O train_trial.zip -c https://cf-my.sharepoint.com/:u:/g/personal/camachocolladosj_cardiff_ac_uk/ERFsG4by92ZPuW1dQQGuLfcBzHifN-NX1tCL6s6g-9-RMw?download=1\n",
        "!wget -O test_images.zip -c https://cf-my.sharepoint.com/:u:/g/personal/camachocolladosj_cardiff_ac_uk/ETXzWJCEdKJDtFluRbYGUGYBzfdnaeOuSwL5hiqCW-k38Q?download=1\n",
        "!wget -O test_data.zip -c 'https://docs.google.com/uc?export=download&id=10vDZsY0EhzvFFR8IF-3P_2ApOF0GIMML&confirm=t'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9dMXXR2vmAI"
      },
      "source": [
        "## Unzip and unify file names for convenience (Ignore disk warning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh6rIioIqY9X"
      },
      "outputs": [],
      "source": [
        "!unzip train_trial.zip\n",
        "!rm train_trial.zip\n",
        "!unzip test_images.zip\n",
        "!rm test_images.zip\n",
        "!unzip test_data.zip -d test_data\n",
        "!rm test_data.zip\n",
        "!mkdir semeval-2023-task-1-V-WSD-train-v1/test_v1\n",
        "!mv test_data/en.test.data.v1.1.txt semeval-2023-task-1-V-WSD-train-v1/test_v1/en.test.data.v1.txt\n",
        "!mv test_data/en.test.gold.v1.1.txt semeval-2023-task-1-V-WSD-train-v1/test_v1/en.test.gold.v1.txt\n",
        "!mv test_data/it.test.data.v1.1.txt semeval-2023-task-1-V-WSD-train-v1/test_v1/it.test.data.v1.txt\n",
        "!mv test_data/it.test.gold.v1.1.txt semeval-2023-task-1-V-WSD-train-v1/test_v1/it.test.gold.v1.txt\n",
        "!mv test_data/fa.test.data.txt semeval-2023-task-1-V-WSD-train-v1/test_v1/fa.test.data.v1.txt\n",
        "!mv test_data/fa.test.gold.txt semeval-2023-task-1-V-WSD-train-v1/test_v1/fa.test.gold.v1.txt\n",
        "!rm -r test_data\n",
        "!mv test_images semeval-2023-task-1-V-WSD-train-v1/test_v1/test_images_v1\n",
        "!mv semeval-2023-task-1-V-WSD-train-v1 semeval-2023-task-1-V-WSD-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load datasets to pandas dataframe"
      ],
      "metadata": {
        "id": "7W7UAfKMAXaG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zd793tkqee0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97470ed1-fca3-4708-e220-11d1e47e2862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train data...\n",
            "Done; train data loaded (12869 rows)!\n",
            "Loading trial data...\n",
            "Done; trial data loaded (16 rows)!\n",
            "Loading en.test data...\n",
            "Done; en.test data loaded (463 rows)!\n",
            "Loading it.test data...\n",
            "Done; it.test data loaded (305 rows)!\n",
            "Loading fa.test data...\n",
            "Done; fa.test data loaded (200 rows)!\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_data_frame(parts):\n",
        "    data_frames = []\n",
        "    for part in parts:\n",
        "        print(f\"Loading {part} data...\")\n",
        "        df1 = pd.read_csv(\n",
        "            f'semeval-2023-task-1-V-WSD-v1/{part if \"test\" not in part else \"test\"}_v1/{part}.data.v1.txt', \n",
        "            sep='\\t', \n",
        "            header=None\n",
        "        )\n",
        "        df2 = pd.read_csv(\n",
        "            f'semeval-2023-task-1-V-WSD-v1/{part if \"test\" not in part else \"test\"}_v1/{part}.gold.v1.txt', \n",
        "            sep='\\t', \n",
        "            header=None\n",
        "        )\n",
        "\n",
        "        df1.rename(\n",
        "            columns={\n",
        "                0: 'word', \n",
        "                1: 'phrase', \n",
        "                2: 'image_0_name', \n",
        "                3: 'image_1_name', \n",
        "                4: 'image_2_name', \n",
        "                5: 'image_3_name', \n",
        "                6: 'image_4_name', \n",
        "                7: 'image_5_name', \n",
        "                8: 'image_6_name', \n",
        "                9: 'image_7_name', \n",
        "                10: 'image_8_name', \n",
        "                11: 'image_9_name'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df2.rename(\n",
        "            columns={\n",
        "                0: 'gold_name'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "\n",
        "        # TODO: Add columns \"image_{i}\" to \"df1\" which are images' path. (~6 lines)\n",
        "        directory = f\"/content/semeval-2023-task-1-V-WSD-v1/{part if 'test' not in part else 'test'}_v1/{part if 'test' not in part else 'test'}_images_v1/\"\n",
        "        for ind in range(10):\n",
        "          df1[f\"image_{ind}\"] = directory + df1[f'image_{ind}_name']\n",
        "\n",
        "\n",
        "        df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
        "        df[f'labels'] = df.apply(lambda x: [i for i in range(10) if x[f'image_{i}_name'] == x['gold_name']][0], axis=1)\n",
        "        data_frames.append(df)\n",
        "        print(f\"Done; {part} data loaded ({len(df)} rows)!\")\n",
        "    return data_frames\n",
        "\n",
        "train_df, trial_df, en_test_df, it_test_df, fa_test_df = load_dataset_data_frame(['train', 'trial', 'en.test', 'it.test', 'fa.test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUbFOLYp3T9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0637d147-0ef7-4888-95a3-17436beaa78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/semeval-2023-task-1-V-WSD-v1/trial_v1/trial_images_v1/image.68.jpg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              word                         phrase   image_0_name  \\\n",
              "0        andromeda                 andromeda tree  image.155.jpg   \n",
              "1           angora                    angora city    image.5.jpg   \n",
              "2         anteater             marsupial anteater  image.147.jpg   \n",
              "3             bank                   bank erosion  image.104.jpg   \n",
              "4           router                internet router  image.127.jpg   \n",
              "5            stick                   centre stick  image.100.jpg   \n",
              "6            swing                      swing hit   image.51.jpg   \n",
              "7             tube                    london tube  image.105.jpg   \n",
              "8            venus                  venus surface   image.60.jpg   \n",
              "9            wheel                 breaking wheel  image.111.jpg   \n",
              "10           white                     white yolk  image.130.jpg   \n",
              "11      acrobatics           acrobatics maneuvers  image.120.jpg   \n",
              "12          adalia                 biology adalia   image.44.jpg   \n",
              "13  administration  administration prime minister   image.39.jpg   \n",
              "14           amber                   amber bijoux   image.10.jpg   \n",
              "15        ambrosia                  ambrosia food   image.31.jpg   \n",
              "\n",
              "     image_1_name   image_2_name   image_3_name   image_4_name   image_5_name  \\\n",
              "0    image.68.jpg    image.9.jpg   image.72.jpg  image.158.jpg   image.86.jpg   \n",
              "1    image.52.jpg   image.96.jpg   image.70.jpg   image.46.jpg   image.91.jpg   \n",
              "2    image.16.jpg  image.107.jpg  image.135.jpg   image.93.jpg   image.59.jpg   \n",
              "3    image.64.jpg  image.108.jpg   image.80.jpg   image.21.jpg   image.99.jpg   \n",
              "4     image.0.jpg   image.20.jpg   image.18.jpg  image.112.jpg   image.97.jpg   \n",
              "5    image.62.jpg  image.156.jpg   image.78.jpg  image.122.jpg   image.81.jpg   \n",
              "6   image.141.jpg   image.11.jpg   image.77.jpg   image.95.jpg   image.33.jpg   \n",
              "7   image.129.jpg   image.41.jpg   image.43.jpg  image.102.jpg   image.28.jpg   \n",
              "8    image.37.jpg   image.83.jpg   image.94.jpg   image.17.jpg   image.29.jpg   \n",
              "9    image.69.jpg   image.82.jpg   image.73.jpg   image.74.jpg   image.48.jpg   \n",
              "10  image.154.jpg   image.71.jpg   image.45.jpg  image.136.jpg   image.42.jpg   \n",
              "11  image.133.jpg   image.58.jpg  image.126.jpg   image.57.jpg  image.134.jpg   \n",
              "12    image.2.jpg   image.22.jpg  image.152.jpg   image.66.jpg  image.116.jpg   \n",
              "13  image.109.jpg  image.119.jpg  image.153.jpg   image.90.jpg   image.49.jpg   \n",
              "14   image.85.jpg  image.128.jpg  image.159.jpg  image.142.jpg  image.144.jpg   \n",
              "15   image.84.jpg  image.103.jpg  image.125.jpg  image.151.jpg   image.61.jpg   \n",
              "\n",
              "     image_6_name   image_7_name  ...  \\\n",
              "0     image.7.jpg  image.132.jpg  ...   \n",
              "1    image.76.jpg  image.139.jpg  ...   \n",
              "2    image.88.png  image.131.jpg  ...   \n",
              "3   image.117.jpg  image.146.jpg  ...   \n",
              "4    image.24.jpg    image.1.jpg  ...   \n",
              "5   image.148.jpg  image.114.jpg  ...   \n",
              "6    image.65.jpg  image.113.jpg  ...   \n",
              "7    image.79.jpg  image.138.jpg  ...   \n",
              "8    image.32.jpg  image.137.jpg  ...   \n",
              "9   image.140.jpg  image.118.jpg  ...   \n",
              "10    image.4.jpg   image.38.jpg  ...   \n",
              "11   image.67.jpg   image.30.jpg  ...   \n",
              "12   image.25.jpg   image.47.jpg  ...   \n",
              "13   image.75.jpg   image.15.jpg  ...   \n",
              "14   image.35.jpg   image.23.jpg  ...   \n",
              "15  image.101.jpg   image.98.jpg  ...   \n",
              "\n",
              "                                              image_2  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_3  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_4  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_5  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_6  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_7  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_8  \\\n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   \n",
              "\n",
              "                                              image_9      gold_name labels  \n",
              "0   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.86.jpg      5  \n",
              "1   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.70.jpg      3  \n",
              "2   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.107.jpg      2  \n",
              "3   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.64.jpg      1  \n",
              "4   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.18.jpg      3  \n",
              "5   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.156.jpg      2  \n",
              "6   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.54.jpg      9  \n",
              "7   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.28.jpg      5  \n",
              "8   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.124.jpg      9  \n",
              "9   /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.118.jpg      7  \n",
              "10  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.71.jpg      2  \n",
              "11  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.134.jpg      5  \n",
              "12  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.25.jpg      6  \n",
              "13  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.149.jpg      9  \n",
              "14  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...   image.85.jpg      1  \n",
              "15  /content/semeval-2023-task-1-V-WSD-v1/trial_v1...  image.103.jpg      2  \n",
              "\n",
              "[16 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e38777b7-2045-4ca2-ba9e-c48c1b963769\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>phrase</th>\n",
              "      <th>image_0_name</th>\n",
              "      <th>image_1_name</th>\n",
              "      <th>image_2_name</th>\n",
              "      <th>image_3_name</th>\n",
              "      <th>image_4_name</th>\n",
              "      <th>image_5_name</th>\n",
              "      <th>image_6_name</th>\n",
              "      <th>image_7_name</th>\n",
              "      <th>...</th>\n",
              "      <th>image_2</th>\n",
              "      <th>image_3</th>\n",
              "      <th>image_4</th>\n",
              "      <th>image_5</th>\n",
              "      <th>image_6</th>\n",
              "      <th>image_7</th>\n",
              "      <th>image_8</th>\n",
              "      <th>image_9</th>\n",
              "      <th>gold_name</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>andromeda</td>\n",
              "      <td>andromeda tree</td>\n",
              "      <td>image.155.jpg</td>\n",
              "      <td>image.68.jpg</td>\n",
              "      <td>image.9.jpg</td>\n",
              "      <td>image.72.jpg</td>\n",
              "      <td>image.158.jpg</td>\n",
              "      <td>image.86.jpg</td>\n",
              "      <td>image.7.jpg</td>\n",
              "      <td>image.132.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.86.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>angora</td>\n",
              "      <td>angora city</td>\n",
              "      <td>image.5.jpg</td>\n",
              "      <td>image.52.jpg</td>\n",
              "      <td>image.96.jpg</td>\n",
              "      <td>image.70.jpg</td>\n",
              "      <td>image.46.jpg</td>\n",
              "      <td>image.91.jpg</td>\n",
              "      <td>image.76.jpg</td>\n",
              "      <td>image.139.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.70.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anteater</td>\n",
              "      <td>marsupial anteater</td>\n",
              "      <td>image.147.jpg</td>\n",
              "      <td>image.16.jpg</td>\n",
              "      <td>image.107.jpg</td>\n",
              "      <td>image.135.jpg</td>\n",
              "      <td>image.93.jpg</td>\n",
              "      <td>image.59.jpg</td>\n",
              "      <td>image.88.png</td>\n",
              "      <td>image.131.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.107.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bank</td>\n",
              "      <td>bank erosion</td>\n",
              "      <td>image.104.jpg</td>\n",
              "      <td>image.64.jpg</td>\n",
              "      <td>image.108.jpg</td>\n",
              "      <td>image.80.jpg</td>\n",
              "      <td>image.21.jpg</td>\n",
              "      <td>image.99.jpg</td>\n",
              "      <td>image.117.jpg</td>\n",
              "      <td>image.146.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.64.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>router</td>\n",
              "      <td>internet router</td>\n",
              "      <td>image.127.jpg</td>\n",
              "      <td>image.0.jpg</td>\n",
              "      <td>image.20.jpg</td>\n",
              "      <td>image.18.jpg</td>\n",
              "      <td>image.112.jpg</td>\n",
              "      <td>image.97.jpg</td>\n",
              "      <td>image.24.jpg</td>\n",
              "      <td>image.1.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.18.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>stick</td>\n",
              "      <td>centre stick</td>\n",
              "      <td>image.100.jpg</td>\n",
              "      <td>image.62.jpg</td>\n",
              "      <td>image.156.jpg</td>\n",
              "      <td>image.78.jpg</td>\n",
              "      <td>image.122.jpg</td>\n",
              "      <td>image.81.jpg</td>\n",
              "      <td>image.148.jpg</td>\n",
              "      <td>image.114.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.156.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>swing</td>\n",
              "      <td>swing hit</td>\n",
              "      <td>image.51.jpg</td>\n",
              "      <td>image.141.jpg</td>\n",
              "      <td>image.11.jpg</td>\n",
              "      <td>image.77.jpg</td>\n",
              "      <td>image.95.jpg</td>\n",
              "      <td>image.33.jpg</td>\n",
              "      <td>image.65.jpg</td>\n",
              "      <td>image.113.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.54.jpg</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tube</td>\n",
              "      <td>london tube</td>\n",
              "      <td>image.105.jpg</td>\n",
              "      <td>image.129.jpg</td>\n",
              "      <td>image.41.jpg</td>\n",
              "      <td>image.43.jpg</td>\n",
              "      <td>image.102.jpg</td>\n",
              "      <td>image.28.jpg</td>\n",
              "      <td>image.79.jpg</td>\n",
              "      <td>image.138.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.28.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>venus</td>\n",
              "      <td>venus surface</td>\n",
              "      <td>image.60.jpg</td>\n",
              "      <td>image.37.jpg</td>\n",
              "      <td>image.83.jpg</td>\n",
              "      <td>image.94.jpg</td>\n",
              "      <td>image.17.jpg</td>\n",
              "      <td>image.29.jpg</td>\n",
              "      <td>image.32.jpg</td>\n",
              "      <td>image.137.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.124.jpg</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wheel</td>\n",
              "      <td>breaking wheel</td>\n",
              "      <td>image.111.jpg</td>\n",
              "      <td>image.69.jpg</td>\n",
              "      <td>image.82.jpg</td>\n",
              "      <td>image.73.jpg</td>\n",
              "      <td>image.74.jpg</td>\n",
              "      <td>image.48.jpg</td>\n",
              "      <td>image.140.jpg</td>\n",
              "      <td>image.118.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.118.jpg</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>white</td>\n",
              "      <td>white yolk</td>\n",
              "      <td>image.130.jpg</td>\n",
              "      <td>image.154.jpg</td>\n",
              "      <td>image.71.jpg</td>\n",
              "      <td>image.45.jpg</td>\n",
              "      <td>image.136.jpg</td>\n",
              "      <td>image.42.jpg</td>\n",
              "      <td>image.4.jpg</td>\n",
              "      <td>image.38.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.71.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>acrobatics</td>\n",
              "      <td>acrobatics maneuvers</td>\n",
              "      <td>image.120.jpg</td>\n",
              "      <td>image.133.jpg</td>\n",
              "      <td>image.58.jpg</td>\n",
              "      <td>image.126.jpg</td>\n",
              "      <td>image.57.jpg</td>\n",
              "      <td>image.134.jpg</td>\n",
              "      <td>image.67.jpg</td>\n",
              "      <td>image.30.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.134.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>adalia</td>\n",
              "      <td>biology adalia</td>\n",
              "      <td>image.44.jpg</td>\n",
              "      <td>image.2.jpg</td>\n",
              "      <td>image.22.jpg</td>\n",
              "      <td>image.152.jpg</td>\n",
              "      <td>image.66.jpg</td>\n",
              "      <td>image.116.jpg</td>\n",
              "      <td>image.25.jpg</td>\n",
              "      <td>image.47.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.25.jpg</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>administration</td>\n",
              "      <td>administration prime minister</td>\n",
              "      <td>image.39.jpg</td>\n",
              "      <td>image.109.jpg</td>\n",
              "      <td>image.119.jpg</td>\n",
              "      <td>image.153.jpg</td>\n",
              "      <td>image.90.jpg</td>\n",
              "      <td>image.49.jpg</td>\n",
              "      <td>image.75.jpg</td>\n",
              "      <td>image.15.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.149.jpg</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>amber</td>\n",
              "      <td>amber bijoux</td>\n",
              "      <td>image.10.jpg</td>\n",
              "      <td>image.85.jpg</td>\n",
              "      <td>image.128.jpg</td>\n",
              "      <td>image.159.jpg</td>\n",
              "      <td>image.142.jpg</td>\n",
              "      <td>image.144.jpg</td>\n",
              "      <td>image.35.jpg</td>\n",
              "      <td>image.23.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.85.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>ambrosia</td>\n",
              "      <td>ambrosia food</td>\n",
              "      <td>image.31.jpg</td>\n",
              "      <td>image.84.jpg</td>\n",
              "      <td>image.103.jpg</td>\n",
              "      <td>image.125.jpg</td>\n",
              "      <td>image.151.jpg</td>\n",
              "      <td>image.61.jpg</td>\n",
              "      <td>image.101.jpg</td>\n",
              "      <td>image.98.jpg</td>\n",
              "      <td>...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>/content/semeval-2023-task-1-V-WSD-v1/trial_v1...</td>\n",
              "      <td>image.103.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e38777b7-2045-4ca2-ba9e-c48c1b963769')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e38777b7-2045-4ca2-ba9e-c48c1b963769 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e38777b7-2045-4ca2-ba9e-c48c1b963769');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "column_name = 'image_1'  # Replace with the name of the column you want to display\n",
        "row_index = 0  # Replace with the index of the row you want to display\n",
        "\n",
        "value = trial_df.loc[row_index, column_name]\n",
        "print(value)\n",
        "trial_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create HuggingFace dataset from pandas dataframe"
      ],
      "metadata": {
        "id": "hmDF4L0HAgOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Gr6qY3xuXx"
      },
      "outputs": [],
      "source": [
        "from datasets import Features, Sequence\n",
        "# TODO: Specify each column datatype, refer to the output of the last cell (use \"Features\" class) (~1 line)\n",
        "features = Features({\n",
        "  'word': Value(dtype='string', id=None),\n",
        "  'phrase': Value(dtype='string', id=None),\n",
        "  'image_0_name': Value(dtype='string', id=None),\n",
        "  'image_1_name': Value(dtype='string', id=None),\n",
        "  'image_2_name': Value(dtype='string', id=None),\n",
        "  'image_3_name': Value(dtype='string', id=None),\n",
        "  'image_4_name': Value(dtype='string', id=None),\n",
        "  'image_5_name': Value(dtype='string', id=None),\n",
        "  'image_6_name': Value(dtype='string', id=None),\n",
        "  'image_7_name': Value(dtype='string', id=None),\n",
        "  'image_8_name': Value(dtype='string', id=None),\n",
        "  'image_9_name': Value(dtype='string', id=None),\n",
        "  'image_0': Value(dtype='string', id=None),\n",
        "  'image_1': Value(dtype='string', id=None),\n",
        "  'image_2': Value(dtype='string', id=None),\n",
        "  'image_3': Value(dtype='string', id=None),\n",
        "  'image_4': Value(dtype='string', id=None),\n",
        "  'image_5': Value(dtype='string', id=None),\n",
        "  'image_6': Value(dtype='string', id=None),\n",
        "  'image_7': Value(dtype='string', id=None),\n",
        "  'image_8': Value(dtype='string', id=None),\n",
        "  'image_9': Value(dtype='string', id=None),\n",
        "  'gold_name': Value(dtype='string', id=None),\n",
        "  'labels': ClassLabel(num_classes=10, names=[str(i)for i in range(10)], names_file=None, id=None),\n",
        "})\n",
        "# end of TODO\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df, features=features).shuffle(seed=GLOBAL_SEED).select(range(496))\n",
        "trial_dataset = Dataset.from_pandas(trial_df, features=features).shuffle(seed=GLOBAL_SEED) # Whole trial dataset\n",
        "en_test_dataset = Dataset.from_pandas(en_test_df, features=features).shuffle(seed=GLOBAL_SEED).select(range(64))\n",
        "it_test_dataset = Dataset.from_pandas(it_test_df, features=features).shuffle(seed=GLOBAL_SEED).select(range(64))\n",
        "fa_test_dataset = Dataset.from_pandas(fa_test_df, features=features).shuffle(seed=GLOBAL_SEED).select(range(64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHEZzwYk3WMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8853c9-0556-4780-b5dd-6af7c058138e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['word', 'phrase', 'image_0_name', 'image_1_name', 'image_2_name', 'image_3_name', 'image_4_name', 'image_5_name', 'image_6_name', 'image_7_name', 'image_8_name', 'image_9_name', 'image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5', 'image_6', 'image_7', 'image_8', 'image_9', 'gold_name', 'labels'],\n",
              "    num_rows: 16\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "trial_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data using AltCLIP processor"
      ],
      "metadata": {
        "id": "IWrohwRUAwga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_woaw(phrase, word) -> str:\n",
        "    # TODO: Omit the word from phrase (~1 line) / don't forget the space around it\n",
        "    woaw = phrase.replace(f\" {word} \", \" \")\n",
        "    # end of TODO\n",
        "    return woaw"
      ],
      "metadata": {
        "id": "UUkKkM8V82vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPgL-C5lzupu"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n",
        "\n",
        "def process_function(examples):\n",
        "    # TODO: Call processor and pass the pharse, WOAW (use \"omit_ambiguous_word\") and images as its arguments\n",
        "    # Use return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64 (~1 line)\n",
        "    woaw = get_woaw(examples['phrase'], examples['word'])\n",
        "    print(examples['image_1'])\n",
        "    imgs = [PIL.Image.open(examples[f'image_{i}']) for i in range(10)]\n",
        "    processor_output = processor(text=[examples['phrase'], woaw] , images=imgs, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
        "    # end of TODO\n",
        "\n",
        "    processor_output['phrase_input_ids'] = processor_output['input_ids'][:1,:]\n",
        "    processor_output['phrase_attention_mask'] = processor_output['attention_mask'][:1,:]\n",
        "    processor_output['woaw_input_ids'] = processor_output['input_ids'][1:,:]\n",
        "    processor_output['woaw_attention_mask'] = processor_output['attention_mask'][1:,:]\n",
        "    processor_output.pop('input_ids')\n",
        "    processor_output.pop('attention_mask')\n",
        "    return processor_output\n",
        "\n",
        "processed_train_dataset = train_dataset.map(process_function, writer_batch_size=32, cache_file_name='alt_train_dataset_cache')\n",
        "processed_trial_dataset = trial_dataset.map(process_function, writer_batch_size=32, cache_file_name='alt_trial_dataset_cache')\n",
        "processed_en_test_dataset = en_test_dataset.map(process_function, writer_batch_size=32, cache_file_name='alt_en_test_dataset_cache')\n",
        "processed_it_test_dataset = it_test_dataset.map(process_function, writer_batch_size=32, cache_file_name='alt_it_test_dataset_cache')\n",
        "processed_fa_test_dataset = fa_test_dataset.map(process_function, writer_batch_size=32, cache_file_name='alt_fa_test_dataset_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSVVlt0m3YLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e40fc2-c737-40f4-9299-12cabe52bf69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['word', 'phrase', 'image_0_name', 'image_1_name', 'image_2_name', 'image_3_name', 'image_4_name', 'image_5_name', 'image_6_name', 'image_7_name', 'image_8_name', 'image_9_name', 'image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5', 'image_6', 'image_7', 'image_8', 'image_9', 'gold_name', 'labels', 'pixel_values', 'phrase_input_ids', 'phrase_attention_mask', 'woaw_input_ids', 'woaw_attention_mask'],\n",
              "    num_rows: 16\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "processed_trial_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract and save features using AltCLIP"
      ],
      "metadata": {
        "id": "vgdZBmW5A1SG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0y-81iDS8KA"
      },
      "outputs": [],
      "source": [
        "datasets_dict = {\n",
        "    'train': concatenate_datasets([processed_train_dataset, processed_trial_dataset]).shuffle(seed=GLOBAL_SEED),\n",
        "    'en_test': processed_en_test_dataset,\n",
        "    'it_test': processed_it_test_dataset,\n",
        "    'fa_test': processed_fa_test_dataset\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(dataset, batch_size, i):\n",
        "    # TODO: Return the i-th batch from dataset (~1 line)\n",
        "    batch = dataset.select(i * batch_size)\n",
        "    #end of TODO\n",
        "    return batch\n",
        "\n",
        "def remove_second_dim(tensor):\n",
        "    # TODO: remove the second dim of tensor (~1 line) / squeez\n",
        "    tensor = torch.squeeze(tensor, dim=1)\n",
        "    # end of TODO\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "t-3YEDz89zls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmQ-fDIvSv0m"
      },
      "outputs": [],
      "source": [
        "def extract_features(datasets_dict, batch_size=1):\n",
        "    model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\").to('cuda:0')\n",
        "    for processed_dataset_name, processed_dataset in datasets_dict.items():\n",
        "        features_directory = f'alt_{processed_dataset_name}_features'\n",
        "        if os.path.exists(features_directory):\n",
        "            shutil.rmtree(features_directory, ignore_errors=True)\n",
        "        os.mkdir(features_directory)\n",
        "        processed_dataset.set_format(type='torch', columns=['phrase_input_ids', 'phrase_attention_mask', 'woaw_input_ids', 'woaw_attention_mask', 'pixel_values', 'labels'])\n",
        "        for i in tqdm(range(0, processed_dataset.num_rows, batch_size)):\n",
        "            batch = get_batch(processed_dataset, batch_size, i)[:]\n",
        "            phrase_input_ids = remove_second_dim(batch['phrase_input_ids'])\n",
        "            phrase_attention_mask = remove_second_dim(batch['phrase_attention_mask'])\n",
        "            woaw_input_ids = remove_second_dim(batch['woaw_input_ids'])\n",
        "            woaw_attention_mask = remove_second_dim(batch['woaw_attention_mask'])\n",
        "            batch_size, image_count, image_dim0, image_dim1, image_dim2 = batch['pixel_values'].shape\n",
        "            # TODO: Reshape batch['pixel_values'] to (batch_size * image_count, image_dim0, image_dim1, image_dim2) (~1 line)\n",
        "            pixel_values = torch.reshape(batch['pixel_values'], (batch_size * image_count, image_dim0, image_dim1, image_dim2))\n",
        "            # end of TODO\n",
        "\n",
        "            # TODO: Move phrase_input_ids, phrase_attention_mask, woaw_input_ids, woaw_attention_mask, pixel_values to GPU (~5 lines)\n",
        "            phrase_input_ids = phrase_input_ids.to('cuda:0')\n",
        "            phrase_attention_mask = phrase_attention_mask.to('cuda:0')\n",
        "            woaw_input_ids = woaw_input_ids.to('cuda:0')\n",
        "            woaw_attention_mask = woaw_attention_mask.to('cuda:0')\n",
        "            pixel_values = pixel_values.to('cuda:0')\n",
        "            # end of TODO\n",
        "            \n",
        "            torch.save(torch.unsqueeze(model.get_text_features(phrase_input_ids, phrase_attention_mask), dim=1), f'{features_directory}/phrase_features_{i}.pt')\n",
        "            torch.save(torch.unsqueeze(model.get_text_features(woaw_input_ids, woaw_attention_mask), dim=1), f'{features_directory}/woaw_features_{i}.pt')\n",
        "            torch.save(torch.reshape(model.get_image_features(pixel_values), (batch_size, image_count, -1)), f'{features_directory}/image_features_{i}.pt')\n",
        "    \n",
        "extract_features(datasets_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add exctracted features to HuggingFace datasets"
      ],
      "metadata": {
        "id": "KmFZX-pcBGv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart Your Runtime Here to Avoid Out of Memory Issue!**"
      ],
      "metadata": {
        "id": "W1D2Y1hTczBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "yb6gyUa3oncG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import requirements after restart again"
      ],
      "metadata": {
        "id": "zvY_oLALB1yH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QmjY39XdD2W"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from PIL import Image as PIL_Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import evaluate\n",
        "\n",
        "\n",
        "from datasets import Dataset, Features, Value, Image, ClassLabel, concatenate_datasets\n",
        "from transformers import AltCLIPProcessor, AltCLIPModel\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.models.altclip.modeling_altclip import AltCLIPEncoderLayer\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.modeling_outputs import BaseModelOutput, ModelOutput\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set some parameters to have determinism after restart again"
      ],
      "metadata": {
        "id": "yzO7iGk_B_WJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c824898-a106-4baf-c82d-4cc3ec1434a0",
        "id": "kC5ByuWydD2W"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
          ]
        }
      ],
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "PIL_Image.MAX_IMAGE_PIXELS = 933120000\n",
        "\n",
        "GLOBAL_SEED = 10\n",
        "\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "torch.manual_seed(GLOBAL_SEED)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load datasets from cache files"
      ],
      "metadata": {
        "id": "rfmvBQOeBhlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_dataset = Dataset.from_file('alt_train_dataset_cache')\n",
        "processed_trial_dataset = Dataset.from_file('alt_trial_dataset_cache')\n",
        "processed_en_test_dataset = Dataset.from_file('alt_en_test_dataset_cache')\n",
        "processed_it_test_dataset = Dataset.from_file('alt_it_test_dataset_cache')\n",
        "processed_fa_test_dataset = Dataset.from_file('alt_fa_test_dataset_cache')"
      ],
      "metadata": {
        "id": "6Omgwyjqm-Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add features to datasets"
      ],
      "metadata": {
        "id": "eHpL0wDdBpCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_dict = {\n",
        "    'train': concatenate_datasets([processed_train_dataset, processed_trial_dataset]).shuffle(seed=GLOBAL_SEED),\n",
        "    'en_test': processed_en_test_dataset,\n",
        "    'it_test': processed_it_test_dataset,\n",
        "    'fa_test': processed_fa_test_dataset\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g74GVtFEc_tC",
        "outputId": "051e94b8-5d2c-4d93-e667-6f53ed7b6ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /content/cache-bbfa129cc0dc97af.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Bi287jSyRu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "a4520340c0924e988dfe9bd60102c7d4",
            "ccc5c62dd55f478182a454fd9b44f394",
            "9e2791881f934e45858f1ccaec668500",
            "0ad71a8d98284b65b0cd9a8c95fbdaa8",
            "3dc2c0af3e214c2183e18d9b05fc2f79",
            "c92ffa0c5098437aa81f36b6c7dd6b18",
            "238361c4230d4486a40927ffedaf7145",
            "9783bbba7cdf4469ace81ba616b15fd0",
            "1c5fd5c6a638468c8bb7d4e0eead2199",
            "db9df2a1e19f42eda6c6410806dd4bf8",
            "e1cd13236e1f43aa9aa3c574835f3fb3"
          ]
        },
        "outputId": "a2a15939-1012-4f83-ed34-18bb5e7eb494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:13<00:00, 39.35it/s] \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Flattening the indices:   0%|          | 0/512 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4520340c0924e988dfe9bd60102c7d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 670.59it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 990.66it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 918.90it/s]\n"
          ]
        }
      ],
      "source": [
        "def add_features_to_datasets(datasets_dict, batch_size=1):\n",
        "    for processed_dataset_name, processed_dataset in datasets_dict.items():\n",
        "        datasets_dict[processed_dataset_name] = datasets_dict[processed_dataset_name].remove_columns([\n",
        "            'phrase_input_ids', 'phrase_attention_mask', 'woaw_input_ids', 'woaw_attention_mask', 'pixel_values'\n",
        "        ])\n",
        "        features_directory = f'alt_{processed_dataset_name}_features'\n",
        "        phrase_features = []\n",
        "        woaw_features = []\n",
        "        image_features = []\n",
        "        for i in tqdm(range(0, processed_dataset.num_rows, batch_size)):\n",
        "            phrase_features.append(torch.load(f'{features_directory}/phrase_features_{i}.pt'))\n",
        "            woaw_features.append(torch.load(f'{features_directory}/woaw_features_{i}.pt'))\n",
        "            image_features.append(torch.load(f'{features_directory}/image_features_{i}.pt'))\n",
        "        phrase_features = torch.cat(phrase_features, dim=0)\n",
        "        woaw_features = torch.cat(woaw_features, dim=0)\n",
        "        image_features = torch.cat(image_features, dim=0)\n",
        "\n",
        "        shutil.rmtree(features_directory, ignore_errors=True)\n",
        "\n",
        "        # TODO: Add phrase_features, woaw_features, image_features to datasets_dict[processed_dataset_name] (~3 lines)\n",
        "        datasets_dict[processed_dataset_name] = datasets_dict[processed_dataset_name].add_column(\"phrase_features\", phrase_features)\n",
        "        datasets_dict[processed_dataset_name] = datasets_dict[processed_dataset_name].add_column(\"woaw_features\", woaw_features)\n",
        "        datasets_dict[processed_dataset_name] = datasets_dict[processed_dataset_name].add_column(\"image_features\", image_features)\n",
        "\n",
        "        # end of TODO\n",
        "\n",
        "\n",
        "add_features_to_datasets(datasets_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bciQehMPtrP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090c854f-4bdd-461d-edb6-e96138490627"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['word', 'phrase', 'image_0_name', 'image_1_name', 'image_2_name', 'image_3_name', 'image_4_name', 'image_5_name', 'image_6_name', 'image_7_name', 'image_8_name', 'image_9_name', 'image_0', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5', 'image_6', 'image_7', 'image_8', 'image_9', 'gold_name', 'labels', 'image_features', 'phrase_features', 'woaw_features'],\n",
              "    num_rows: 512\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "datasets_dict['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate"
      ],
      "metadata": {
        "id": "C3-9vodUCEqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct model"
      ],
      "metadata": {
        "id": "aKrY-H6wCNvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model config class"
      ],
      "metadata": {
        "id": "VEhQ6aUyCWU_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6Otoya1ykqK"
      },
      "outputs": [],
      "source": [
        "class RAltCLIPConfig(PretrainedConfig):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size=768,\n",
        "        intermediate_size=3072,\n",
        "        num_hidden_layers=3,\n",
        "        num_attention_heads=8,\n",
        "        hidden_act=\"quick_gelu\",\n",
        "        layer_norm_eps=1e-05,\n",
        "        attention_dropout=0.0,\n",
        "        num_images_to_rank=10,\n",
        "        logit_scale_init_value=2.6592,\n",
        "        loss_func=\"CE\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.hidden_act = hidden_act\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.num_images_to_rank = num_images_to_rank\n",
        "        self.logit_scale_init_value = logit_scale_init_value\n",
        "        self.loss_func = loss_func"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder class"
      ],
      "metadata": {
        "id": "5D4O5S4hCbMp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrTtduBKyoe9"
      },
      "outputs": [],
      "source": [
        "# Copied from transformers.models.clip.modeling_altclip.AltCLIPEncoder with AltCLIP->RAltCLIP\n",
        "class RAltCLIPEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n",
        "    [`AltCLIPEncoderLayer`].\n",
        "    Args:\n",
        "        config: AltCLIPConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: RAltCLIPConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList([AltCLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs_embeds,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutput]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
        "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "                than the model's internal embedding lookup matrix.\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Causal mask for the text model. Mask values selected in `[0, 1]`:\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        hidden_states = inputs_embeds\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(encoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    causal_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = encoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    causal_attention_mask,\n",
        "                    output_attentions=output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output data class"
      ],
      "metadata": {
        "id": "MtOsDu58Cfmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQehiD-6yuyJ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RAltCLIPOutput(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model class"
      ],
      "metadata": {
        "id": "unrAcDq8Cl6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgGcyeWTyw_M"
      },
      "outputs": [],
      "source": [
        "class RAltCLIPModel(PreTrainedModel):\n",
        "    \"\"\"Relative AltCLIP\"\"\"\n",
        "\n",
        "    config_class = RAltCLIPConfig\n",
        "    \n",
        "    def __init__(self, config: RAltCLIPConfig):\n",
        "        super().__init__(config)\n",
        "        self.rank_encoder = RAltCLIPEncoder(config)\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * config.logit_scale_init_value)\n",
        "        self.num_images_to_rank = config.num_images_to_rank\n",
        "        self.loss_func = config.loss_func\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image_features: torch.FloatTensor,\n",
        "        phrase_features: torch.FloatTensor,\n",
        "        woaw_features: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "    ) -> RAltCLIPOutput:\n",
        "        if woaw_features != None:\n",
        "            phrase_image_embeds = torch.cat((phrase_features, image_features, woaw_features), dim=1)\n",
        "        else:\n",
        "            phrase_image_embeds = torch.cat((phrase_features, image_features), dim=1)\n",
        "\n",
        "        # TODO: Pass phrase_image_embeds to rank_encoder and get sum of all hidden states (~2 lines)\n",
        "        rank_encoder_output = None\n",
        "        hidden_states_sum = None\n",
        "        # end of TODO\n",
        "        \n",
        "        # normalized features\n",
        "        hidden_states_sum = hidden_states_sum / hidden_states_sum.norm(p=2, dim=-1, keepdim=True)\n",
        "        \n",
        "        phrase_embeds_sum = hidden_states_sum[:,0,:].unsqueeze(1)\n",
        "        image_embeds_sum = hidden_states_sum[:,1:11,:]\n",
        "        \n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_phrase = torch.matmul(phrase_embeds_sum, torch.transpose(image_embeds_sum, -1, -2)) * logit_scale\n",
        "        logits = logits_per_phrase.squeeze(1)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.loss_func == 'CE':\n",
        "                output_loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = output_loss_fct(logits.view(-1, self.num_images_to_rank), labels.view(-1))\n",
        "            elif self.loss_func == 'SIM':\n",
        "                _, _, hidden_size = phrase_embeds_sum.shape\n",
        "                similarity_loss_fct = nn.CosineEmbeddingLoss()\n",
        "                similarity_labels = torch.nn.functional.one_hot(labels, self.num_images_to_rank)\n",
        "                similarity_labels = torch.where(similarity_labels == 1, 1, -1)\n",
        "                loss = similarity_loss_fct(\n",
        "                    torch.repeat_interleave(phrase_embeds_sum, self.num_images_to_rank, dim=1).reshape(-1, hidden_size), \n",
        "                    image_embeds_sum.reshape(-1, hidden_size), \n",
        "                    similarity_labels.reshape(-1)\n",
        "                )\n",
        "            \n",
        "        return RAltCLIPOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load evaluation metrics"
      ],
      "metadata": {
        "id": "adL0R0SyCJOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw1st-evu0q_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "85f7c7b1f4884fff851463c97ce445d7",
            "c9198934d70643e9ad4b018b62295eb5",
            "eaae99b1d2414cfcba454de6f3f38589",
            "b021fb7d985942c1b14518e264ab7423",
            "9e6138ff40b34c519db335290e649af1",
            "a6e47d36b7d449788d3a53d1f7991512",
            "6cbee4b0bf6f435a83b35dd95f06e474",
            "450f4ec3af444fa68077be39ffe8ae85",
            "b0a73b93e60f41b089ff558a27b15646",
            "0e847fcb46ed46b787ea4853c8778f9c",
            "bc871af6fac74d418db2f2efb219c21e",
            "b5494e1745a745d88d038561b6d1ce5a",
            "71955713967c4e38b9f63754203d4232",
            "8f6c998afbb54387b4c803fdc738cbc2",
            "62073eaf271c4f3f829a533326cf3650",
            "fb57ccf13ef640b4b39e18642bb98caa",
            "19c3461da0134ac289ac11598a3a305c",
            "4e7cf8403a2b43af9a2b471e5845a436",
            "1adfe79dce2c45d2af04a3851c3a7ebc",
            "59506ff08dad4670bd2cb7156c227e25",
            "0626dbc1c46e43bc8be4f15f0e34c4c4",
            "75d8d5bb6c6f433cb66a13b9d61228e9"
          ]
        },
        "outputId": "b8b18b2d-9d2b-405a-9b85-2a4fe2a4221a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85f7c7b1f4884fff851463c97ce445d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5494e1745a745d88d038561b6d1ce5a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "accuracy_metric = evaluate.load('accuracy')\n",
        "mrr_metric = evaluate.load('posicube/mean_reciprocal_rank')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions.argmax(axis=-1), references=labels)\n",
        "    mrr = mrr_metric.compute(predictions=np.where(np.argsort(-predictions, axis=1) == np.expand_dims(labels, axis=1))[1])\n",
        "    return {'accuracy': accuracy['accuracy'], 'mrr': mrr['mrr']}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate"
      ],
      "metadata": {
        "id": "Hfy-n2iOCSHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsp7Ss-qzk3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "76996088-25ca-4952-bfdc-a6bdae13240a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.060500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.960400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL: RAltCLIP_CE_12_T\n",
            "EN: Acc: 0.578125, MRR: 0.7354724702380954\n",
            "IT: Acc: 0.28125, MRR: 0.4773809523809523\n",
            "FA: Acc: 0.09375, MRR: 0.3039682539682539\n",
            "OVERAL: Acc: 0.3177083333333333, MRR: 0.5056072255291005\n"
          ]
        }
      ],
      "source": [
        "model_name = 'RAltCLIP_CE_12_T'\n",
        "\n",
        "def model_init():\n",
        "    config = RAltCLIPConfig(dropout=0.5, attention_dropout=0.5, loss_func='CE')\n",
        "    model = RAltCLIPModel(config)\n",
        "    return model\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    logging_strategy='epoch',\n",
        "    logging_steps=1,\n",
        "    save_strategy='epoch',\n",
        "    save_steps=1,\n",
        "    metric_for_best_model='accuracy',\n",
        "    per_device_train_batch_size=256,\n",
        "    num_train_epochs=3,\n",
        "    seed=GLOBAL_SEED,\n",
        "    weight_decay=0.2\n",
        ")\n",
        "\n",
        "# TODO: Set the argumetns \n",
        "trainer = Trainer(\n",
        "    model_init=None,\n",
        "    args=None,\n",
        "    train_dataset=None,\n",
        "    compute_metrics=None,\n",
        ")\n",
        "# end of TODO\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "eval_en_result = trainer.evaluate(datasets_dict['en_test'])\n",
        "eval_it_result = trainer.evaluate(datasets_dict['it_test'])\n",
        "eval_fa_result = trainer.evaluate(datasets_dict['fa_test'])\n",
        "\n",
        "print(f\"MODEL: {model_name}\")\n",
        "print(f\"EN: Acc: {eval_en_result['eval_accuracy']}, MRR: {eval_en_result['eval_mrr']}\")\n",
        "print(f\"IT: Acc: {eval_it_result['eval_accuracy']}, MRR: {eval_it_result['eval_mrr']}\")\n",
        "print(f\"FA: Acc: {eval_fa_result['eval_accuracy']}, MRR: {eval_fa_result['eval_mrr']}\")\n",
        "print(f\"OVERAL: Acc: {(eval_en_result['eval_accuracy']+eval_it_result['eval_accuracy']+eval_fa_result['eval_accuracy'])/3}, MRR: {(eval_en_result['eval_mrr']+eval_it_result['eval_mrr']+eval_fa_result['eval_mrr'])/3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2KrjOa_a_K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4520340c0924e988dfe9bd60102c7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccc5c62dd55f478182a454fd9b44f394",
              "IPY_MODEL_9e2791881f934e45858f1ccaec668500",
              "IPY_MODEL_0ad71a8d98284b65b0cd9a8c95fbdaa8"
            ],
            "layout": "IPY_MODEL_3dc2c0af3e214c2183e18d9b05fc2f79"
          }
        },
        "ccc5c62dd55f478182a454fd9b44f394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c92ffa0c5098437aa81f36b6c7dd6b18",
            "placeholder": "​",
            "style": "IPY_MODEL_238361c4230d4486a40927ffedaf7145",
            "value": "Flattening the indices:   0%"
          }
        },
        "9e2791881f934e45858f1ccaec668500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9783bbba7cdf4469ace81ba616b15fd0",
            "max": 512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c5fd5c6a638468c8bb7d4e0eead2199",
            "value": 512
          }
        },
        "0ad71a8d98284b65b0cd9a8c95fbdaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9df2a1e19f42eda6c6410806dd4bf8",
            "placeholder": "​",
            "style": "IPY_MODEL_e1cd13236e1f43aa9aa3c574835f3fb3",
            "value": " 0/512 [00:00&lt;?, ? examples/s]"
          }
        },
        "3dc2c0af3e214c2183e18d9b05fc2f79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "c92ffa0c5098437aa81f36b6c7dd6b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238361c4230d4486a40927ffedaf7145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9783bbba7cdf4469ace81ba616b15fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5fd5c6a638468c8bb7d4e0eead2199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db9df2a1e19f42eda6c6410806dd4bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1cd13236e1f43aa9aa3c574835f3fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85f7c7b1f4884fff851463c97ce445d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9198934d70643e9ad4b018b62295eb5",
              "IPY_MODEL_eaae99b1d2414cfcba454de6f3f38589",
              "IPY_MODEL_b021fb7d985942c1b14518e264ab7423"
            ],
            "layout": "IPY_MODEL_9e6138ff40b34c519db335290e649af1"
          }
        },
        "c9198934d70643e9ad4b018b62295eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6e47d36b7d449788d3a53d1f7991512",
            "placeholder": "​",
            "style": "IPY_MODEL_6cbee4b0bf6f435a83b35dd95f06e474",
            "value": "Downloading builder script: 100%"
          }
        },
        "eaae99b1d2414cfcba454de6f3f38589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450f4ec3af444fa68077be39ffe8ae85",
            "max": 4203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a73b93e60f41b089ff558a27b15646",
            "value": 4203
          }
        },
        "b021fb7d985942c1b14518e264ab7423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e847fcb46ed46b787ea4853c8778f9c",
            "placeholder": "​",
            "style": "IPY_MODEL_bc871af6fac74d418db2f2efb219c21e",
            "value": " 4.20k/4.20k [00:00&lt;00:00, 166kB/s]"
          }
        },
        "9e6138ff40b34c519db335290e649af1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e47d36b7d449788d3a53d1f7991512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cbee4b0bf6f435a83b35dd95f06e474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "450f4ec3af444fa68077be39ffe8ae85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a73b93e60f41b089ff558a27b15646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e847fcb46ed46b787ea4853c8778f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc871af6fac74d418db2f2efb219c21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5494e1745a745d88d038561b6d1ce5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71955713967c4e38b9f63754203d4232",
              "IPY_MODEL_8f6c998afbb54387b4c803fdc738cbc2",
              "IPY_MODEL_62073eaf271c4f3f829a533326cf3650"
            ],
            "layout": "IPY_MODEL_fb57ccf13ef640b4b39e18642bb98caa"
          }
        },
        "71955713967c4e38b9f63754203d4232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c3461da0134ac289ac11598a3a305c",
            "placeholder": "​",
            "style": "IPY_MODEL_4e7cf8403a2b43af9a2b471e5845a436",
            "value": "Downloading builder script: 100%"
          }
        },
        "8f6c998afbb54387b4c803fdc738cbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1adfe79dce2c45d2af04a3851c3a7ebc",
            "max": 3324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59506ff08dad4670bd2cb7156c227e25",
            "value": 3324
          }
        },
        "62073eaf271c4f3f829a533326cf3650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0626dbc1c46e43bc8be4f15f0e34c4c4",
            "placeholder": "​",
            "style": "IPY_MODEL_75d8d5bb6c6f433cb66a13b9d61228e9",
            "value": " 3.32k/3.32k [00:00&lt;00:00, 205kB/s]"
          }
        },
        "fb57ccf13ef640b4b39e18642bb98caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c3461da0134ac289ac11598a3a305c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7cf8403a2b43af9a2b471e5845a436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1adfe79dce2c45d2af04a3851c3a7ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59506ff08dad4670bd2cb7156c227e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0626dbc1c46e43bc8be4f15f0e34c4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d8d5bb6c6f433cb66a13b9d61228e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}