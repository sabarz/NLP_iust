{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCb3cVwzrfbb",
        "outputId": "e892fc48-d837-4782-af7d-9851f9f8e3db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "SOURCE_DIR = 'gdrive/MyDrive/ACL/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pfDri3MGtFm-"
      },
      "outputs": [],
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UXy2OcR7s84V"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  words_cosine_similarity = dict()\n",
        "  for token in embedding_dict.keys():\n",
        "    words_cosine_similarity[token] = cos(embedding_dict[word], embedding_dict[token]).item()\n",
        "  words_cosine_similarity = dict(sorted(words_cosine_similarity.items(), key=lambda item: item[1]))\n",
        "  return list(words_cosine_similarity.keys())[-k:][::-1]\n",
        "\n",
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mf-3eSz6udw1"
      },
      "outputs": [],
      "source": [
        "word = 'ولنتاین'\n",
        "k = 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "#0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0FjQ9_ZMkve",
        "outputId": "4bdcb081-fb2d-4b09-f488-a57e56ae848b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting json-lines\n",
            "  Downloading json_lines-0.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from json-lines) (1.15.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VHj8JyLUMnSo"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from files and save them in memory base on each year.\n",
        "# 2. remove urls, hashtags and usernames."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "#1. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPqc0I0yuNlI"
      },
      "outputs": [],
      "source": [
        "# 1. find one hot encoding of each word for each year\n",
        "# 2. find 10 nearest words from \"ولنتاین\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "#2. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJMju0Tiw9YA"
      },
      "outputs": [],
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "# 2. choose one tweets randomly.\n",
        "# 3. find 10 nearest tweets from chosen tweet."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jIuLL3Mn2sLA"
      },
      "source": [
        "#3. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCnxqaVY2zCc"
      },
      "outputs": [],
      "source": [
        "# 1. train a word2vec model base on all tweets for each year.\n",
        "# 2. find 10 nearest words from \"ولنتاین\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "#4. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc8hBCnn4cV_"
      },
      "outputs": [],
      "source": [
        "# 1. fine tune a bert model base on all tweets for each year.\n",
        "# 2. find 10 nearest words from \"ولنتاین\"\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"This is an example sentence.\"\n",
        "\n",
        "# Tokenize input sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n",
        "\n",
        "# Convert input to PyTorch tensor\n",
        "input_tensor = torch.tensor([input_ids])\n",
        "\n",
        "# Generate contextualized embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "\n",
        "# Extract contextualized embedding for each token\n",
        "token_embeddings = embeddings.squeeze(0)\n",
        "\n",
        "# Print token-level embeddings\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"Token: {token}\\tEmbedding: {token_embeddings[i]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
