{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-09T12:57:47.166849Z","iopub.execute_input":"2023-05-09T12:57:47.167192Z","iopub.status.idle":"2023-05-09T12:57:47.182688Z","shell.execute_reply.started":"2023-05-09T12:57:47.167162Z","shell.execute_reply":"2023-05-09T12:57:47.181611Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/iust-vqa/image_features.pickle\n/kaggle/input/iust-vqa/answer_list.txt\n/kaggle/input/iust-vqa/val.csv\n/kaggle/input/iust-vqa/train.csv\n/kaggle/input/iust-vqa/test.csv\n/kaggle/input/iust-vqa/image_question.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:57:47.185389Z","iopub.execute_input":"2023-05-09T12:57:47.186203Z","iopub.status.idle":"2023-05-09T12:57:47.191139Z","shell.execute_reply.started":"2023-05-09T12:57:47.186158Z","shell.execute_reply":"2023-05-09T12:57:47.190244Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\nbert = BertModel.from_pretrained(\"bert-base-uncased\")\nembedding_matrix = bert.embeddings.word_embeddings.weight","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:58:58.203169Z","iopub.execute_input":"2023-05-09T12:58:58.203968Z","iopub.status.idle":"2023-05-09T12:59:00.071363Z","shell.execute_reply.started":"2023-05-09T12:58:58.203926Z","shell.execute_reply":"2023-05-09T12:59:00.070476Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"PAD_ID = 0\nCLS_ID = 101\ndevice = \"cuda:0\"","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:59:08.102095Z","iopub.execute_input":"2023-05-09T12:59:08.102447Z","iopub.status.idle":"2023-05-09T12:59:08.111082Z","shell.execute_reply.started":"2023-05-09T12:59:08.102402Z","shell.execute_reply":"2023-05-09T12:59:08.110012Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_text = \"Here is some text to encode\"\ninput_ids = tokenizer.encode(input_text, add_special_tokens=True)\n# you can get BERT embeddings like this:\nembedding_matrix[input_ids].shape, input_ids","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:59:10.791945Z","iopub.execute_input":"2023-05-09T12:59:10.792303Z","iopub.status.idle":"2023-05-09T12:59:10.801615Z","shell.execute_reply.started":"2023-05-09T12:59:10.792268Z","shell.execute_reply":"2023-05-09T12:59:10.800470Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(torch.Size([9, 768]), [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102])"},"metadata":{}}]},{"cell_type":"code","source":"#Let's begin !\nfrom torch.utils.data import Dataset\nimport pickle\nimport json\nimport csv \nimport torch\n\nclass VQADataset(Dataset):\n\n    def __init__(self, split_path):\n        image_features_path = \"/kaggle/input/iust-vqa/image_features.pickle\"\n        answers_list_path = \"/kaggle/input/iust-vqa/answer_list.txt\"\n        image2questions_path = \"/kaggle/input/iust-vqa/image_question.json\"\n        \n        ## Read image features, use pickle!\n        with open(image_features_path, 'rb') as f:\n            ### YOUR CODE HERE\n            self.image_features = pickle.load(f)\n            ### YOUR CODE HERE\n            \n        \n        ##sample: self.question2img[q_id] = img_id\n        self.question2img = {}\n        \n        ##sample: self.questions[q_id] = {\"text\" : q_text, \"tokenized\" : tokenized_question}\n        ## tokenization: tokenizer.encode(sentence)\n        self.questions = {}\n        \n        with open(image2questions_path, 'r') as f:\n            ## YOUR CODE HERE\n            ## Load json file (image2questions)\n            data = json.load(f)\n            \n            ## retrieve requested values \"self.question2img\", \"self.questions\" from givenn json\n            ## ~ 6 lines\n            for img_id in data:\n                for question in data[img_id]:\n                    q_id = question[0]\n                    q_text = question[1]\n                    \n                    self.questions[q_id] = {\"text\" : q_text , \"tokenized\" : tokenizer.encode(q_text)}\n                    self.question2img[q_id] = img_id\n            ### YOUR CODE HERE\n        \n        self.possible_answers = []\n        with open(answers_list_path, 'r') as f:\n            ## read answers list from text file, save them in an array\n            self.possible_answers = f.read().split()\n        \n        ## sample: self.data[idx] = q_id\n        self.data = []\n        ## sample: self.labels[idx] = 4\n        self.labels = []\n        \n        \n        \n        ## load data from \"split_path\", fill self.data and self.labels as requested! take a look at train.csv\n        # https://docs.python.org/3/library/csv.html#csv.DictReader\n        with open(split_path, newline='') as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                self.data.append(int(row['question_id']))\n                if (row['label'] is not None):\n                    self.labels.append(int(row['label']))\n                else:\n                    self.labels = None\n\n    def __getitem__(self, idx):\n        \"\"\"This method returns tuple of (question_id, image_features (Tensor), tokenized_question (Tensor), label\n        \n        Note: label can be None!\n        \"\"\"\n        \n        \n        q_id = self.data[idx]\n        ### YOUR CODE HERE\n        ## WARNING: while making tensors, DO NOT FORGET TO USE .to(device) at the end!\n\n        q_tokenized = self.questions[q_id][\"tokenized\"]\n        img_id = self.question2img[q_id]\n        label = None\n        if(self.labels is not None):\n            label = self.labels[idx]\n        return (q_id , torch.tensor(self.image_features[img_id]).to(device) , torch.tensor(q_tokenized).to(device) ,label)\n        ### YOUR CODE HERE\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:59:14.787891Z","iopub.execute_input":"2023-05-09T12:59:14.788290Z","iopub.status.idle":"2023-05-09T12:59:14.803623Z","shell.execute_reply.started":"2023-05-09T12:59:14.788257Z","shell.execute_reply":"2023-05-09T12:59:14.802454Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n    \"\"\"\n        Batch post processing, we can pad questions! \n        returns q_ids, images (Tensor), questions(Tensor), labels (Tensor)\n    \"\"\"\n    images = []\n    questions = []\n    labels = []\n    q_ids = []\n    l = []\n    ### YOUR CODE HERE\n    ## WARNING: while making tensors, DO NOT FORGET TO USE .to(device) at the end!\n    for q_id, img, q_tokens, label in batch:\n        q_ids.append(q_id)\n        images.append(img)\n        questions.append(q_tokens)\n        \n        if(label is not None):\n            labels.append(torch.tensor(label).to(device))\n        else:\n            labels = None\n    \n    ### Stack images into one tensor\n    ## torch.stack, shape must be (batch_size, img_features)\n    images = torch.stack(images , dim = 0)\n    \n    ## stack labels if they're not None, else make labels None!\n    if labels is not None:\n        labels = torch.stack(labels , dim = 0)\n    \n    ## pad questions, shape must be (batch_size, longest_sentence)\n    ## https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n    questions = pad_sequence(questions, batch_first=True, padding_value=0)\n    \n    \n    return q_ids , images , questions , labels","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:59:18.297573Z","iopub.execute_input":"2023-05-09T12:59:18.297909Z","iopub.status.idle":"2023-05-09T12:59:18.305342Z","shell.execute_reply.started":"2023-05-09T12:59:18.297881Z","shell.execute_reply":"2023-05-09T12:59:18.303258Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndset = VQADataset(\"/kaggle/input/iust-vqa/train.csv\")\ndata_loader_train = DataLoader(dset, collate_fn=collate_batch, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:59:37.396881Z","iopub.execute_input":"2023-05-09T12:59:37.397949Z","iopub.status.idle":"2023-05-09T12:59:37.478880Z","shell.execute_reply.started":"2023-05-09T12:59:37.397901Z","shell.execute_reply":"2023-05-09T12:59:37.478014Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n## Nothing, just look =)))\n\nclass PositionalEncoder(nn.Module):\n    \"\"\"Positional encoding class pulled from the PyTorch documentation tutorial\n    on Transformers for seq2seq models:\n    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoder, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()\\\n                             * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:59:53.778877Z","iopub.execute_input":"2023-05-09T12:59:53.779465Z","iopub.status.idle":"2023-05-09T12:59:53.787202Z","shell.execute_reply.started":"2023-05-09T12:59:53.779410Z","shell.execute_reply":"2023-05-09T12:59:53.786224Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport math\n\n#The most interesting part!\n\nclass VQA_Simple(nn.Module):\n    def __init__(self, dropout, text_hidden_size, n_layers, n_heads, image_hidden_size, n_outputs):\n        super().__init__()\n        self.dropout = dropout\n        self.d_model = text_hidden_size\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.image_hidden_size = image_hidden_size\n        self.PAD = PAD_ID\n        \n        self.embedding_matrix = bert.embeddings.word_embeddings.weight\n        \n        \n        ##initilize TransformerEncoderLayer\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n        encoder_layer = torch.nn.TransformerEncoderLayer(d_model = self.d_model, \n                                                         nhead = self.n_heads,\n                                                         dropout = self.dropout)\n        \n        ##initilize TransformerEncoder\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n        self.t_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers = self.n_layers)\n        \n        ##if you looke enough, you can initilize positional encoder!!\n        self.pe = PositionalEncoder(d_model = self.d_model , dropout = self.dropout)\n        \n        ##initilize TransformerDecoderLayer\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html\n        decoder_layer = torch.nn.TransformerDecoderLayer(d_model = self.d_model , nhead = self.n_heads)\n        \n        ##initilize TransformerDecoder\n        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html\n        self.t_decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers = self.n_layers)\n        \n        ##Linear output, recieves concatenation of text and image features, outputs final answer!\n        self.linear = torch.nn.Linear(self.d_model +  image_hidden_size, n_outputs )\n        \n    def forward(self, images, input_ids):\n        ##images shape: (batch_size, img_features)\n        ##input_ids shape: (batch_size, sequence_len)\n        b_size = images.shape[0]\n        \n        ### YOUR CODE HERE\n        \n        ## Calculate masks, shape: (batch_size, sequence_len)\n        src_key_mask = (input_ids == self.PAD)\n        ##embeddings of the given input_ids, extracted from self.embedding_matrix\n        ##shape should be (sequence_len, batch_size, text_embedding_features)\n        embeddings = self.embedding_matrix[input_ids].permute(1, 0, 2)\n        \n        ##Positional embeddings\n        ##shape should be (sequence_len, batch_size, text_embedding_features)\n        positional_embeddings = self.pe(embeddings)\n        \n        ## feed positinal_embeddings to the encoder!\n        ## output shape should be (sequence_len, batch_size, d_model)\n        ## additional args:  src_key_padding_mask\n        encoder_output = self.t_encoder(positional_embeddings, src_key_padding_mask = src_key_mask)\n        \n        ##(batch_size, 1)\n        tgt = torch.tensor([CLS_ID] * b_size).unsqueeze(1).to(device)\n        ##(batch_size, 1)\n        tgt_key_padding_mask = (tgt == self.PAD)\n        \n        ##embeddings of the given input_ids, extracted from self.embedding_matrix\n        ##shape should be (1, batch_size, text_embedding_features)\n        tgt_embeddings = self.embedding_matrix[tgt].permute(1, 0, 2)\n        \n\n        # target attention masks to avoid future tokens in our predictions\n        # Adapted from PyTorch source code:\n        # https://github.com/pytorch/pytorch/blob/176174a68ba2d36b9a5aaef0943421682ecc66d4/torch/nn/modules/transformer.py#L130\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(1).to(device)\n        \n        ## Positional embedding \n        tgt_positions = tgt_embeddings + self.pe(tgt_embeddings)\n        \n        output = self.t_decoder(tgt=tgt_positions, \n                                memory=encoder_output,\n                                tgt_mask=tgt_mask,\n                                tgt_key_padding_mask = tgt_key_padding_mask, \n                                memory_key_padding_mask = src_key_mask) ##(1, batch_size, text_embedding_features)\n        \n        \n        \n        output_text = output.permute(1, 0, 2).squeeze(1) ## (batch_size, text_embedding_features)\n        \n        ##https://pytorch.org/docs/stable/generated/torch.cat.html\n        #concatenate text output and image features\n        concatenated = torch.cat((output_text, images), dim=1)\n        \n        \n        y = self.linear(concatenated)\n        \n        return y\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:15:38.659541Z","iopub.execute_input":"2023-05-09T13:15:38.660043Z","iopub.status.idle":"2023-05-09T13:15:38.673405Z","shell.execute_reply.started":"2023-05-09T13:15:38.660010Z","shell.execute_reply":"2023-05-09T13:15:38.672410Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmodel = VQA_Simple(dropout=0.1, \n                   text_hidden_size=768, \n                   n_layers=2, \n                   n_heads=6, \n                   image_hidden_size=512, \n                   n_outputs=10).to(device)\nlr=1e-4\nepochs = 25\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(list(model.parameters()), lr=lr)\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (q_ids, images, questions, labels) in enumerate(pbar := tqdm(data_loader_train, total=len(data_loader_train))):\n        pbar.set_description(f\"Epoch {epoch}\")\n        \n        optimizer.zero_grad()\n        output = model(images, questions)\n        \n        loss = criterion(output, labels)\n        running_loss += loss\n        \n        loss.backward()\n        optimizer.step()\n        log_interval = 5\n        pbar.set_postfix(loss=running_loss/(i+1))\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:21:13.974730Z","iopub.execute_input":"2023-05-09T13:21:13.975091Z","iopub.status.idle":"2023-05-09T13:21:43.444280Z","shell.execute_reply.started":"2023-05-09T13:21:13.975060Z","shell.execute_reply":"2023-05-09T13:21:43.443350Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Epoch 0: 100%|██████████| 25/25 [00:01<00:00, 20.85it/s, loss=tensor(2.4924, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 1: 100%|██████████| 25/25 [00:01<00:00, 21.95it/s, loss=tensor(2.2903, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 2: 100%|██████████| 25/25 [00:01<00:00, 21.69it/s, loss=tensor(2.2802, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 3: 100%|██████████| 25/25 [00:01<00:00, 21.77it/s, loss=tensor(2.2705, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 4: 100%|██████████| 25/25 [00:01<00:00, 22.19it/s, loss=tensor(2.1900, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 5: 100%|██████████| 25/25 [00:01<00:00, 21.95it/s, loss=tensor(1.8134, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 6: 100%|██████████| 25/25 [00:01<00:00, 19.02it/s, loss=tensor(1.3203, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 7: 100%|██████████| 25/25 [00:01<00:00, 18.57it/s, loss=tensor(1.0635, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 8: 100%|██████████| 25/25 [00:01<00:00, 21.78it/s, loss=tensor(0.9933, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 9: 100%|██████████| 25/25 [00:01<00:00, 22.13it/s, loss=tensor(0.8364, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 10: 100%|██████████| 25/25 [00:01<00:00, 21.70it/s, loss=tensor(0.7350, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 11: 100%|██████████| 25/25 [00:01<00:00, 21.67it/s, loss=tensor(0.6986, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 12: 100%|██████████| 25/25 [00:01<00:00, 21.92it/s, loss=tensor(0.6193, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 13: 100%|██████████| 25/25 [00:01<00:00, 21.78it/s, loss=tensor(0.6072, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 14: 100%|██████████| 25/25 [00:01<00:00, 21.98it/s, loss=tensor(0.6046, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 15: 100%|██████████| 25/25 [00:01<00:00, 21.84it/s, loss=tensor(0.5017, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 16: 100%|██████████| 25/25 [00:01<00:00, 20.64it/s, loss=tensor(0.5508, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 17: 100%|██████████| 25/25 [00:01<00:00, 22.18it/s, loss=tensor(0.5285, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 18: 100%|██████████| 25/25 [00:01<00:00, 22.01it/s, loss=tensor(0.3966, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 19: 100%|██████████| 25/25 [00:01<00:00, 22.19it/s, loss=tensor(0.3819, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 20: 100%|██████████| 25/25 [00:01<00:00, 21.83it/s, loss=tensor(0.3720, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 21: 100%|██████████| 25/25 [00:01<00:00, 22.25it/s, loss=tensor(0.3330, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 22: 100%|██████████| 25/25 [00:01<00:00, 21.84it/s, loss=tensor(0.3167, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 23: 100%|██████████| 25/25 [00:01<00:00, 22.15it/s, loss=tensor(0.2934, device='cuda:0', grad_fn=<DivBackward0>)]\nEpoch 24: 100%|██████████| 25/25 [00:01<00:00, 21.79it/s, loss=tensor(0.3328, device='cuda:0', grad_fn=<DivBackward0>)]\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(data_loader, net):\n    predicts = []\n    ids = []\n    net.eval()\n    for i, (q_ids, images, questions, _) in enumerate(pbar := tqdm(data_loader, total=len(data_loader))):\n        outputs = net(images, questions)\n        outputs = torch.argmax(outputs, dim=1)\n        predicts.extend(outputs.cpu().tolist())\n        ids.extend(q_ids)\n    return predicts, ids\n\n\ntest_dset = VQADataset(\"/kaggle/input/iust-vqa/test.csv\")\ndata_loader_test = DataLoader(test_dset, collate_fn=collate_batch, batch_size=8)\npreds, ids = predict(data_loader_test, model)\n\n# with open(\"output.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:22:19.358021Z","iopub.execute_input":"2023-05-09T13:22:19.358705Z","iopub.status.idle":"2023-05-09T13:22:19.562649Z","shell.execute_reply.started":"2023-05-09T13:22:19.358669Z","shell.execute_reply":"2023-05-09T13:22:19.561577Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"100%|██████████| 14/14 [00:00<00:00, 123.36it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\noutput_data = {\"question_id\": [str(id) for id in ids], \"label\": preds}\ndf = pd.DataFrame(output_data)\ndf.to_csv(\"/kaggle/working/output.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T13:22:25.846116Z","iopub.execute_input":"2023-05-09T13:22:25.846491Z","iopub.status.idle":"2023-05-09T13:22:25.854656Z","shell.execute_reply.started":"2023-05-09T13:22:25.846458Z","shell.execute_reply":"2023-05-09T13:22:25.853386Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}